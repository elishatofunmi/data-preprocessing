{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize, WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"Do you know how to tokenization works? It's actually quite interesting\n",
    "! let's analyze a couple of sentences and figure it out\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence tokinizer\n",
    "# divide the input text into sentence tokens\n",
    "print (\"\\n Sentence tokenizer: \")\n",
    "print (sent_tokenize(input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word tokenizer\n",
    "# divide the input text into word tokens\n",
    "print (\"\\nWord tokenizer: \")\n",
    "print (word_tokenize(input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word punct tokenizer: \n",
      "['Do', 'you', 'know', 'how', 'to', 'tokenization', 'works', '?', 'It', \"'\", 's', 'actually', 'quite', 'interesting', '!', 'let', \"'\", 's', 'analyze', 'a', 'couple', 'of', 'sentences', 'and', 'figure', 'it', 'out']\n"
     ]
    }
   ],
   "source": [
    "# WordPunct tokenizer\n",
    "# divide the input text into word tokens using word punct tokenizer\n",
    "print (\"\\nWord punct tokenizer: \")\n",
    "print (WordPunctTokenizer().tokenize(input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting words to their base forms using stemming\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_words = ['writting', 'calves', 'be', 'branded', 'horse','randomize',\n",
    "              'possible','provision','hospital','kept', 'scratchy','code']\n",
    "\n",
    "# create objects for porter, Lancaster, and Snowball stemmers.\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "       INPUT WORD          PORTER       LANCASTER        SNOWBALL \n",
      " ====================================================================\n",
      "        writting            writ            writ            writ\n",
      "          calves            calv            calv            calv\n",
      "              be              be              be              be\n",
      "         branded           brand           brand           brand\n",
      "           horse            hors            hors            hors\n",
      "       randomize          random          random          random\n",
      "        possible         possibl            poss         possibl\n",
      "       provision          provis          provid          provis\n",
      "        hospital          hospit          hospit          hospit\n",
      "            kept            kept            kept            kept\n",
      "        scratchy        scratchi        scratchy        scratchi\n",
      "            code            code             cod            code\n"
     ]
    }
   ],
   "source": [
    "# create a list of stemmer names for display\n",
    "stemmer_names = ['PORTER', 'LANCASTER', 'SNOWBALL']\n",
    "formatted_text = '{: >16}' * (len(stemmer_names) + 1)\n",
    "print ('\\n', formatted_text.format('INPUT WORD', *stemmer_names), '\\n', '='*68)\n",
    "\n",
    "# stem each word and display the output\n",
    "for word in input_words:\n",
    "    output = [word, porter.stem(word), lancaster.stem(word) , snowball.stem(word)]\n",
    "    print (formatted_text.format(*output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert words to their base forms using lemmatization\n",
    "# lemmatization uses vocabulary and morphological analysis of words. It obtains\n",
    "# the base forms by removing the inflectional word ending such as ing or ed.\n",
    "# this base form of any word is known as lemma.\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_words = ['writing', 'calves','be','branded','horse','randomize','possible',\n",
    "              'provision','hospital','kept','scratchy','code']\n",
    "\n",
    "# create lemmatizer objec\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "               INPUT WORD         NOUN LEMMATIZER         VERB LEMMATIZER \n",
      " ===========================================================================\n"
     ]
    }
   ],
   "source": [
    "# create a list of lemmatizer names for display\n",
    "lemmatizer_names = ['NOUN LEMMATIZER', 'VERB LEMMATIZER']\n",
    "formatted_text = '{:>24}' * (len(lemmatizer_names) + 1)\n",
    "print ('\\n', formatted_text.format('INPUT WORD', *lemmatizer_names), '\\n', '='*75)\n",
    "\n",
    "\n",
    "# lemmatize each word and display the output\n",
    "for word in input_words:\n",
    "    #output = [word, lemmatizer.lemmatize(word, pos = 'n'), lemmatizer.lemmatize(word, pos = 'v')]\n",
    "    #print (formatted_text.format(*output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing text data into chunks\n",
    "\n",
    "import numpy as np\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to divide the input text into chunks.\n",
    "\n",
    "# split the input text into chunks, where each chuck contains N words\n",
    "\n",
    "def chunker(input_data, N):\n",
    "    input_words = input_data.split(\" \")\n",
    "    output = []\n",
    "    \n",
    "    # iteraate through the words and divide them into chunks using the input parameter.\n",
    "    \n",
    "    cur_chunk = []\n",
    "    count = 0\n",
    "    for word in input_words:\n",
    "        cur_chunk.append(word)\n",
    "        count += 1\n",
    "        if count == N:\n",
    "            output.append(\" \".join(cur_chunk))\n",
    "            count, cur_chunk = 0, []\n",
    "    output.append(' '.join(cur_chunk))\n",
    "    return output\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # read the first 12000 words from the brown corpus\n",
    "    input_data = ' '.join(brown.words()[:12000])\n",
    "    \n",
    "    # define the number of words in each chunk\n",
    "    chunk_size = 700\n",
    "    chunks = chunker(input_data, chunk_size)\n",
    "    print (\"\\nNumber of text chunks = \", len(chunks), '\\n')\n",
    "    for i , chunk in enumerate(chunks):\n",
    "        print (\"chunk\", i+1, '==>', chunk[:50])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a bag of words model in NLTK\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import brown\n",
    "from text_chunker import chunker\n",
    "\n",
    "input_data = \" \".join(brown.words()[:5400])\n",
    "\n",
    "chunk_size = 800\n",
    "\n",
    "text_chunks = chunker(input_data, chunk_size)\n",
    "\n",
    "# convert the chunks into dictionary items\n",
    "chunks = []\n",
    "for count, chunk in enumerate(text_chunks):\n",
    "    d = {'index': count, 'text': chunk}\n",
    "    chunks.append(d)\n",
    "    \n",
    "# Extract the document term matrix\n",
    "count_vectorizer = CountVectorizer(min_df = 7, max_df = 20)\n",
    "document_term_matrix = count_vectorizer.fit_transform([chunk['text'] for chunk in chunks])\n",
    "\n",
    "# Extract the vocabulary and display it\n",
    "vocabulary = np.array(count_vectorizer.get_feature_names())\n",
    "print (\"\\nVocabulary: \\n\", vocabulary)\n",
    "\n",
    "\n",
    "# Generate the names for display\n",
    "chunk_names = []\n",
    "for i in range(len(text_chunks)):\n",
    "    chunk_names.append(\"Chunk-\" + str(i+1))\n",
    "    \n",
    "# print the document term matrix\n",
    "print (\"\\nDocument term matrix:\")\n",
    "formatted_text = '{:>12}' * (len(chunk_names), '\\n')\n",
    "for word, item in zip(vocabulary, document_term_matrix.T):\n",
    "    # 'item' is a 'csr_matrix' data structure\n",
    "    output = [word] + [str(freq) for freq in item.data]\n",
    "    print (formatted_text.format(*output))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a category predictor\n",
    "# a category predictor is used to predict the category to which a given piece\n",
    "# of text belongs. \n",
    "\n",
    "# inorder to build this predicator we will use a statistic called TermFrequency-inverse\n",
    "# Document Frequency(tf-idf). it helps us to know understand the importance of a given\n",
    "# word to a document in a set of documents.\n",
    "\n",
    "# The term Frequency(tf) is basically a measure of how frequently each word appears\n",
    "# in a given document.\n",
    "\n",
    "# The inverse Document Frequency (idf), is a measure of how unique a word is to \n",
    "# this document in the given set of documents.\n",
    "\n",
    "# we then combine term frequency and inverse document frequency to formulate a feature\n",
    "# vector to categorize documents.\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# Define the category map that will be used for training.\n",
    "category_map = {'talk.politics.misc': 'politics', 'rec.autos': 'Autos', \n",
    "               'rec.spot.hockey': 'Hockey', 'sci.electronics': 'Electronics',\n",
    "               'sci.med': 'Medicine'}\n",
    "\n",
    "# Get the training dataset\n",
    "training_data = fetch_20newsgroups(subset = 'train',categories = category_map.keys(),\n",
    "                                  shuffle = True, random_state = 5)\n",
    "\n",
    "# Build a count vectorizer and extract term counts\n",
    "count_vectorizer = CountVectorizer()\n",
    "train_tc = count_vectorizer.fit_transform(training_data.data)\n",
    "print (\"\\nDimensions of training data: \", train_tc.shape)\n",
    "\n",
    "# create the tf-idf transformer\n",
    "tfidf = TfidfTransformer()\n",
    "train_tfidf = tfidf.fit_transform(train_tc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define test data\n",
    "input_data = [\"you need to be careful with cars when you are driving on slippery roads\",\n",
    "              \"A lot of devices can be operated wirelessly\", \n",
    "             'players need to be careful when they are closed to goal posts',\n",
    "             'political debates help us understand the perspectives of both sides']\n",
    "\n",
    "# train a multinomial Naive Bayes classifier\n",
    "classifier = MultinomialNB().fit(train_tfidf, training_data.target)\n",
    "\n",
    "# Transform input data using count vectorizer\n",
    "input_tc = count_vectorizer.transfrom(input_data)\n",
    "\n",
    "# Transform vectorized data using tfidf transformer\n",
    "input_tfidf = tfidf.transform(input_tc)\n",
    "\n",
    "# predict the output categories\n",
    "predictions = classifier.predict(input_tfidf)\n",
    "\n",
    "# print the outputs\n",
    "for sent, category in zip(input_data, predictions):\n",
    "    print ('\\nInput: ', sent, '\\nPredicted category:', \\\n",
    "          category_map[training_data.target_names[category]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contructing a gender identifier\n",
    "# in this case, we will use the heuristic to construct a feature vector and \n",
    "# use it to train a classifier.\n",
    "# The heuristic that will be used here is the last N letters of a given name.\n",
    "#For example if the name ends with 'ia', it's most likely a female name, such\n",
    "# as Amelia or Genelia. On the other hand, if the name ends with 'rk', it's likely\n",
    "# a male name such as Mark or Clark.\n",
    "\n",
    "import random\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.classify import accuracy as nltk_accuracy\n",
    "from nltk.corpus import names\n",
    "\n",
    "\n",
    "# Extract last N letters from the input word\n",
    "## and that will act as our 'feature'\n",
    "def extract_features(word, N = 2):\n",
    "    last_n_letters  word[-N:]\n",
    "    return {'feature': last_n_letters.lower()}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # create training data using labeled names available in NLTK\n",
    "    male_list = [(name, 'male') for name in names.word('male.txt')]\n",
    "    female_list = [(name, 'female') for name in names.words('female.txt')]\n",
    "    data = (male_list + female_list)\n",
    "    \n",
    "# seed the random number generator\n",
    "random.seed(5)\n",
    "\n",
    "# shuffle the data\n",
    "random.shuffle(data)\n",
    "\n",
    "# create sample names for testing: test data\n",
    "input_names = ['Alexander', 'Danielle', 'David', 'Cheryl']\n",
    "\n",
    "# Define the number of samples used for train and text\n",
    "num_train = int(0.8 * len(data))\n",
    "\n",
    "# Iterate through different lengths to compare the accuracy\n",
    "for i in range(1,6):\n",
    "    print (\"\\nNumber of end letters:\", i)\n",
    "    features = [(extract_features(n, i), gender) for (n, gender) in data]\n",
    "    \n",
    "# separate the data into training ans testing\n",
    "train_data, test_data = features[:num_train], features[num_train:]\n",
    "\n",
    "# build the NaiveBayes Classifier using the training data\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "\n",
    "# compute the accuracy of the classifier\n",
    "accuracy = round(100 * nltk_accuracy(classifier, test_data), 2)\n",
    "print (\"Accuracy = \" + str(accuracy) + '%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a Sentiment analyzer\n",
    "\n",
    "# Sentiment analysis is the process of determininng the sentiment of a given piece\n",
    "# of text. For example, it can be used to determnine whether a movie review is positive\n",
    "# or negative.\n",
    "\n",
    "# we will use Naive Bayes classifier to build this classifier. We first need to extract\n",
    "# all the unique words from the text.\n",
    "\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy as nltk_accuracy\n",
    "\n",
    "# Extract features from the input list words\n",
    "def extract_features(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # load the reviews from the corpus\n",
    "    fileids_pos = movie_reviews.fileids(\"pos\")\n",
    "    fileids_neg = movie_reviews.fileids(\"neg\")\n",
    "    \n",
    "# Extract the features from the reviews\n",
    "feature_pos = [(extract_features(movie_reviews.words(fileids = [f])), 'Positive') for f in fileids_pos]\n",
    "feature_neg= [(extract_features(movie_reviews.words(fileids = [f])), 'Negative') for f in fileids_neg]\n",
    "\n",
    "# Define the train and test split (80% and 20%) \n",
    "threshold = 0.8\n",
    "num_pos = int(threshold * len(features_pos))\n",
    "num_neg = int(threshold * len(features_neg))\n",
    "\n",
    "\n",
    "# create training and training datasets\n",
    "features_train = features_pos[:num_pos] + features_neg[:num_neg]\n",
    "features_test = features_pos[num_pos:] + features_neg[num_neg:]\n",
    "\n",
    "# Print the number of datapoints used\n",
    "print (\"\\nNumber of training datapoints:\", len(features_train))\n",
    "print (\"Number of test datapoints:\", len(features_test))\n",
    "\n",
    "#Train a Naive Bayes classifier\n",
    "classifier = NaiveBayesClassifier.train(features_train)\n",
    "print (\"\\nAccuracy of the classifier:\", nltk_accuracy(classfier, features_test))\n",
    "\n",
    "# print the top N most infomative words\n",
    "N = 15\n",
    "print (\"\\nTop \" + str(N) + \" most informative words:\")\n",
    "for i, item in enumerate(classifier.most_informative_features()):\n",
    "    print (str(i+1) + '. ' + item[0])\n",
    "    if i == N - 1:\n",
    "        break\n",
    "        \n",
    "# Test input movie reviews\n",
    "input_reviews = ['The costumes in this movie were great',\n",
    "                 'I think the story was terrible and the characters were very weak',\n",
    "                'People say that the director of the movie is amazing',\n",
    "                'This is such an idiotic movie. I will not recommend it to anyone.']\n",
    "\n",
    "print (\"\\nMovie review predictions:\")\n",
    "for review in input_reviews:\n",
    "    print (\"\\nReview:\", review)\n",
    "    # compute the probablilties\n",
    "    probabilities = classifier.prob_classify(extract_features(review.split()))\n",
    "\n",
    "    # Pick the maximum value\n",
    "    predicted_sentiment = probabilities.max()\n",
    "    \n",
    "    # print outputs\n",
    "    print (\"Predicted sentiment: \", predicted_sentiment)\n",
    "    print (\"Probability:\", round(probabilities.prob(predicted_sentiment), 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic modeling using Latent Dirichlet Allocation\n",
    "\n",
    "# it is an unsupervised learning algorithm. It helps to organize our documents in an \n",
    "# optimal way, which can then be used for analysis.\n",
    "\n",
    "# Latent Dirichlet Allocation is a topic modelling technique.\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim import models, corpora\n",
    "\n",
    "# load input data\n",
    "def load_data(input_file):\n",
    "    data = []\n",
    "    with open(input_file, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            data.append(line[:-1])\n",
    "            \n",
    "    return data\n",
    "\n",
    "# Define a function to process the input text.\n",
    "# processor function for tokenizing, removing stop\n",
    "# word, and stemming\n",
    "def process(input_text):\n",
    "    # create a regular expression tokenizer\n",
    "    tokenizer = RegexTokenizer(r'\\w+')\n",
    "    \n",
    "    # create a snowball stemmer to stem the tokenized text\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    \n",
    "    # Get the list of stop words to remove the stop words from the input test\n",
    "    # because they don't add information.\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    \n",
    "    # Tokenize the input string\n",
    "    tokens = tokenizer.tokenize(input_text.lower())\n",
    "    \n",
    "    # remove the stop-words\n",
    "    tokens = [x for x in tokens if not x in stop_words]\n",
    "    \n",
    "    \n",
    "    # perform stemming on the tokenized words\n",
    "    tokens_stemmed = [stemmer.stem(x) for x in tokens]\n",
    "    return tokens_stemmed\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # load in put data\n",
    "    data = load_data('data.txt')\n",
    "    \n",
    "    # create a list for sentence tokens\n",
    "    tokens = [process(x) for x in data]\n",
    "    \n",
    "    # create a dictionary based on the sentence tokens\n",
    "    doc_term_mat = [dict_tokens.doc2bow(token) for token in tokens]\n",
    "    \n",
    "    # define the number of topics for the LDA model\n",
    "    num_topics = 2\n",
    "    \n",
    "    # Generate the LDA model\n",
    "    ldamodel = models.ldamodel.LdaModel(doc_term_mat, \n",
    "                                       num_topics = num_topics, id2word = dict_tokens, passes = 25)\n",
    "    \n",
    "    # print top 5 contributing words for each topic\n",
    "    num_words = 5\n",
    "    print (\"\\nTop \" + str(num_words) + \" contributing words to each topic: \")\n",
    "    for item in ldamodel.print_topics(num_topics = num_topics, num_words= num_words):\n",
    "        print (\"\\nTopic\", item[0])\n",
    "        \n",
    "        # print the contributing words along with their relative contributions\n",
    "        list_of_strings = item[1].split(' + ')\n",
    "        for text in list_of_strings:\n",
    "            weight = text.split('*')[0]\n",
    "            word = text.split('*')[1]\n",
    "            print (word, '==>', str(round(float(weight) * 100, 2)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
