{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import mglearn\n",
    "import sklearn as sk\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_data = pd.read_csv(r\"C:\\Users\\ACER\\Desktop\\pyreach\\to pc\\train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95851, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_data.head(2000)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22256635</td>\n",
       "      <td>Nonsense?  kiss off, geek. what I said is true...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27450690</td>\n",
       "      <td>\"\\n\\n Please do not vandalize pages, as you di...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54037174</td>\n",
       "      <td>\"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77493077</td>\n",
       "      <td>Asking some his nationality is a Racial offenc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79357270</td>\n",
       "      <td>The reader here is not going by my say so for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                       comment_text  toxic  \\\n",
       "0  22256635  Nonsense?  kiss off, geek. what I said is true...      1   \n",
       "1  27450690  \"\\n\\n Please do not vandalize pages, as you di...      0   \n",
       "2  54037174  \"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...      0   \n",
       "3  77493077  Asking some his nationality is a Racial offenc...      0   \n",
       "4  79357270  The reader here is not going by my say so for ...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
       "       'insult', 'identity_hate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Nonsense?  kiss off, geek. what I said is true.  I'll have your account terminated.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['comment_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_describe = data[['id','toxic','severe_toxic','obscene','threat','insult','identity_hate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# introducing the bag of words model\n",
    "# the bag of words model allows us to represent text as a numerical fearure vectors."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To construct a bag-of-words model based on the word counts in the respective documents, we can use the CountVectorized class implemented in sklearn. As we will see in the following code section that the CountVectorizer class taeks an array to text data, which can be documents or just sentences, and construct the bag of words model for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for example\n",
    "docs = np.array(['The sun is shinning', 'The weather is sweet', 'The sun is shining and the weather is sweet'])\n",
    "bag = count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 6, 'sun': 4, 'is': 1, 'shinning': 3, 'weather': 7, 'sweet': 5, 'shining': 2, 'and': 0}\n"
     ]
    }
   ],
   "source": [
    "print (count.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 1 0 1 0]\n",
      " [0 1 0 0 0 1 1 1]\n",
      " [1 2 1 0 1 1 2 1]]\n"
     ]
    }
   ],
   "source": [
    "print (bag.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing word relevancy via term frequency-inverse document frequency"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "term frequency-inverse document freqency (tf-idf) can be used to downweight those frequently occuring words in the feature vectors.\n",
    "sklearn implements the transformer TfidTransformer, that takes the raw term frequences from CountVectorizer as input and transform them into tf-idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.    0.39  0.    0.66  0.5   0.    0.39  0.  ]\n",
      " [ 0.    0.43  0.    0.    0.    0.56  0.43  0.56]\n",
      " [ 0.39  0.46  0.39  0.    0.3   0.3   0.46  0.3 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer()\n",
    "np.set_printoptions(precision = 2)\n",
    "print (tfidf.fit_transform(count.fit_transform(docs)).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Nonsense?  kiss off, geek. what I said is true...\n",
       "1       \"\\n\\n Please do not vandalize pages, as you di...\n",
       "2       \"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...\n",
       "3       Asking some his nationality is a Racial offenc...\n",
       "4       The reader here is not going by my say so for ...\n",
       "5           Fried chickens \\n\\nIs dat sum fried chickens?\n",
       "6       Why can you put English for example on some pl...\n",
       "7       Guy Fawkes \\n\\nim a resident in bridgwater and...\n",
       "8       as far as nicknames go this article is embarra...\n",
       "9       Woodland Meadows\\nGood to hear that you correc...\n",
       "10      \"\\n\\nWell I just finished a good bit of editin...\n",
       "11      Discussion should take place on the article ta...\n",
       "12      Uh oh, you called my bluff. I am intimidated b...\n",
       "13      \"\\nWe should also contact the living descendan...\n",
       "14      \" May 2008 (UTC)\\n\\nNotability of Your New Hea...\n",
       "15      \"\\n\\nWhile I agree that this article isn't FA ...\n",
       "16      a Turkish citizen and him having received an a...\n",
       "17      Please explain why censorship of quality addit...\n",
       "18      In any case, this edit war will last forever. ...\n",
       "19      \"\\n\\n \"\"Vandalism\"\" of George Washington \\n\\nW...\n",
       "20      Why hasn't Alitalia been removed rom the allia...\n",
       "21      \"\\n\\n Another AfD stats example \\n\\nI hope you...\n",
       "22      \"\\nI will ;). How about... ah, I've got nothin...\n",
       "23      \":I have moved some tedious detail in \"\"Survey...\n",
       "24      @AnnieHall, what separates this from capitalis...\n",
       "25      .  and its also not random, it was the first c...\n",
       "26      \"\\nThe Graceful Slick....\\nIs non other than a...\n",
       "27      \"====Regarding edits made during December 2 20...\n",
       "28      \"::The section is now called \"\"Discrepancies a...\n",
       "29      \"\\n\\n Smackdown! \\n\\nGood smackdown on Qatar, ...\n",
       "                              ...                        \n",
       "1970    I will also take a look at the infoboxes for t...\n",
       "1971                      Miranda... \\n\\nGood Work! eth01\n",
       "1972                                 Which is Blunderson?\n",
       "1973    Fingers crossed for the News Bot to run withou...\n",
       "1974    False link! \\n\\nhttp://www.icj-cij.org/icjwww/...\n",
       "1975    Whoops \\n\\nDidn't notice that Taiketsu had alr...\n",
       "1976    So the Emmy's don't list her as an official wi...\n",
       "1977    The team name is Great Britain and Northern Ir...\n",
       "1978    I should point out to you that User:MikeMcGD d...\n",
       "1979    Thanks for the thanks - a quick note \\n\\nWhile...\n",
       "1980    Should not - consensus on the drugs seems very...\n",
       "1981    WikiProject Birds February newsletter \\n\\nThe ...\n",
       "1982    \"\\n\\nOh now I realize: you're the same anon wh...\n",
       "1983    About Comics \\n\\nNow that was fast!  I was hun...\n",
       "1984    \"\\n\\nFair use rationale for File:Super Adventu...\n",
       "1985    \"\\n\\n Unclear sentence \\n\\n2005 papal election...\n",
       "1986                                Ok, no hard feelings.\n",
       "1987    They're twins, of a sort, created at the same ...\n",
       "1988    Walam Olum \\n\\nI think the article is improved...\n",
       "1989     JUDENSCWEIN TO THE OVENS !  LIARS ! ALL OF YOU !\n",
       "1990                    REDIRECT Talk:Silent Descent (EP)\n",
       "1991    We have over 400 people who are willing to use...\n",
       "1992    I haven't made any alterations to the article ...\n",
       "1993    By the way, see Commons:Category:Hadrosaurus F...\n",
       "1994    My organ meat is tasty, and it makes a fine sa...\n",
       "1995                                   calm your self boy\n",
       "1996    Reverted the improper removal \\n\\nThe characte...\n",
       "1997    Nobody, and not the tag, is saying that primar...\n",
       "1998          \"\\n\\nJune 2011 Wikification Drive\\n\\n·T·C \"\n",
       "1999    \"\\n\\nThe article says \"\"And Isidore Ndaywel è ...\n",
       "Name: comment_text, Length: 2000, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['comment_text']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The first important step before we build our bag of word model is to clean the text data by stripping it of all unwanted characters. to illustrated why this is important, let us display the last 50 characters from the first document.\n",
    "\n",
    "A  lot contains HTML markup as well as punctuation and other non-letter characters. For simplicity we will now remove all punctuation marks but only keep emotion characters such as \":)\" since those are certainly useful for sentimental analysis.\n",
    "\n",
    "To do this we will implement the regex libarary in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocessor(text):\n",
    "    text = re.sub(\"<[^>]*>\", '', text)\n",
    "    emotions = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    \n",
    "    text = re.sub('[\\W] + ','',text.lower()).join(emotions).replace('_', '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Series.tolist of 0       Nonsense?  kiss off, geek. what I said is true...\n",
      "1       \"\\n\\n Please do not vandalize pages, as you di...\n",
      "2       \"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...\n",
      "3       Asking some his nationality is a Racial offenc...\n",
      "4       The reader here is not going by my say so for ...\n",
      "5           Fried chickens \\n\\nIs dat sum fried chickens?\n",
      "6       Why can you put English for example on some pl...\n",
      "7       Guy Fawkes \\n\\nim a resident in bridgwater and...\n",
      "8       as far as nicknames go this article is embarra...\n",
      "9       Woodland Meadows\\nGood to hear that you correc...\n",
      "10      \"\\n\\nWell I just finished a good bit of editin...\n",
      "11      Discussion should take place on the article ta...\n",
      "12      Uh oh, you called my bluff. I am intimidated b...\n",
      "13      \"\\nWe should also contact the living descendan...\n",
      "14      \" May 2008 (UTC)\\n\\nNotability of Your New Hea...\n",
      "15      \"\\n\\nWhile I agree that this article isn't FA ...\n",
      "16      a Turkish citizen and him having received an a...\n",
      "17      Please explain why censorship of quality addit...\n",
      "18      In any case, this edit war will last forever. ...\n",
      "19      \"\\n\\n \"\"Vandalism\"\" of George Washington \\n\\nW...\n",
      "20      Why hasn't Alitalia been removed rom the allia...\n",
      "21      \"\\n\\n Another AfD stats example \\n\\nI hope you...\n",
      "22      \"\\nI will ;). How about... ah, I've got nothin...\n",
      "23      \":I have moved some tedious detail in \"\"Survey...\n",
      "24      @AnnieHall, what separates this from capitalis...\n",
      "25      .  and its also not random, it was the first c...\n",
      "26      \"\\nThe Graceful Slick....\\nIs non other than a...\n",
      "27      \"====Regarding edits made during December 2 20...\n",
      "28      \"::The section is now called \"\"Discrepancies a...\n",
      "29      \"\\n\\n Smackdown! \\n\\nGood smackdown on Qatar, ...\n",
      "                              ...                        \n",
      "1970    I will also take a look at the infoboxes for t...\n",
      "1971                      Miranda... \\n\\nGood Work! eth01\n",
      "1972                                 Which is Blunderson?\n",
      "1973    Fingers crossed for the News Bot to run withou...\n",
      "1974    False link! \\n\\nhttp://www.icj-cij.org/icjwww/...\n",
      "1975    Whoops \\n\\nDidn't notice that Taiketsu had alr...\n",
      "1976    So the Emmy's don't list her as an official wi...\n",
      "1977    The team name is Great Britain and Northern Ir...\n",
      "1978    I should point out to you that User:MikeMcGD d...\n",
      "1979    Thanks for the thanks - a quick note \\n\\nWhile...\n",
      "1980    Should not - consensus on the drugs seems very...\n",
      "1981    WikiProject Birds February newsletter \\n\\nThe ...\n",
      "1982    \"\\n\\nOh now I realize: you're the same anon wh...\n",
      "1983    About Comics \\n\\nNow that was fast!  I was hun...\n",
      "1984    \"\\n\\nFair use rationale for File:Super Adventu...\n",
      "1985    \"\\n\\n Unclear sentence \\n\\n2005 papal election...\n",
      "1986                                Ok, no hard feelings.\n",
      "1987    They're twins, of a sort, created at the same ...\n",
      "1988    Walam Olum \\n\\nI think the article is improved...\n",
      "1989     JUDENSCWEIN TO THE OVENS !  LIARS ! ALL OF YOU !\n",
      "1990                    REDIRECT Talk:Silent Descent (EP)\n",
      "1991    We have over 400 people who are willing to use...\n",
      "1992    I haven't made any alterations to the article ...\n",
      "1993    By the way, see Commons:Category:Hadrosaurus F...\n",
      "1994    My organ meat is tasty, and it makes a fine sa...\n",
      "1995                                   calm your self boy\n",
      "1996    Reverted the improper removal \\n\\nThe characte...\n",
      "1997    Nobody, and not the tag, is saying that primar...\n",
      "1998          \"\\n\\nJune 2011 Wikification Drive\\n\\n·T·C \"\n",
      "1999    \"\\n\\nThe article says \"\"And Isidore Ndaywel è ...\n",
      "Name: comment_text, Length: 2000, dtype: object>\n"
     ]
    }
   ],
   "source": [
    "DataFrame_to_list = data['comment_text'].tolist\n",
    "print (DataFrame_to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The reader here is not going by my say so for ethereal vocal style and dark lyrical content. The cited sources in the External Links are saying those things. If you feel the sources are unreliable or I did not represent what they said correctly rewrite or delete it.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['comment_text'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#data['comment_text'] = data['comment_text'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " processing documents into tokens\n",
    "\n",
    "having successfully prepared the dataset, we now need to think about how\n",
    "to split the text corpora into individual elements. One way to tokenize\n",
    "documents is to split them into individual words by spliting the cleaned \n",
    "document at its whitespace characters\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "in the context of tokenization, another useful techinque is word stemming which is the process of transforming a word into its root form that allows us to map related words to the same stem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runners', 'like', 'running', 'and', 'this', 'they', 'run']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "tokenizer('runners like running and this they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['tokenized_comment'] = data['comment_text'].apply(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'not',\n",
       " 'vandalize',\n",
       " 'pages,',\n",
       " 'as',\n",
       " 'you',\n",
       " 'did',\n",
       " 'with',\n",
       " 'this',\n",
       " 'edit',\n",
       " 'to',\n",
       " 'W.',\n",
       " 'S.',\n",
       " 'Merwin.',\n",
       " 'If',\n",
       " 'you',\n",
       " 'continue',\n",
       " 'to',\n",
       " 'do',\n",
       " 'so,',\n",
       " 'you',\n",
       " 'will',\n",
       " 'be',\n",
       " 'blocked',\n",
       " 'from',\n",
       " 'editing.',\n",
       " '\"']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tokenized_comment'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['tokenized_comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    solution = [porter.stem(word) for word in text]\n",
    "    return solution\n",
    "\n",
    "\n",
    "data['tokenized_porter_data'] =data['tokenized_comment'].apply(tokenizer_porter)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Before we jump to the next section where we will train a machine learning model using the bag-of-words model, let us briefly talk about another useful topic called STOP-WORD REMOVAL. Stop words are simply those words that are extremely common in all sorts of texts and are likely bear no (or only little) useful information that can be used to distinguish between different classes of documents. Examples of stop words are is, and , has and the like. Removing stop words can be useful if we are working with raw or normlized term frequencies rather than tf-idfs which are already downweighting frequently occuring words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training a logistic regression model for document classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def re_apply(text):\n",
    "    solution = [w for w in text if w not in stop]\n",
    "    return solution\n",
    "\n",
    "data['tokenized_re_apply'] = data['tokenized_porter_data'].apply(re_apply)\n",
    "data['tokenized_re_apply'][0]== data['tokenized_porter_data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiment = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
       "       'insult', 'identity_hate', 'tokenized_comment', 'tokenized_porter_data',\n",
       "       'tokenized_re_apply'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = data.loc[:1499, 'tokenized_re_apply']\n",
    "y_train = data.loc[:1499, sentiment]\n",
    "x_test = data.loc[1500:,'tokenized_re_apply']\n",
    "y_text = data.loc[1500:, sentiment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(strip_accents = None, lowercase = False, preprocessor = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = [{'vect_ngram_range':[(1,1)], \n",
    "              'vect_stop_words':[stop, None],\n",
    "              'vect_tokenizer': [tokenizer, tokenizer_porter],\n",
    "              'clf_penalty':['l1','l2'],\n",
    "              'clf_C': [1.0,10.0,100.0]},\n",
    "             {'vect_ngram_range':[(1,1)],\n",
    "             'vect_stop_words':[stop, None],\n",
    "             'vect_tokenizer':[tokenizer,tokenizer_porter],\n",
    "             'vect_use_idf':[False],\n",
    "             'vect_norm':[None],\n",
    "             'clf_penalty':['l1','l2'],\n",
    "             'clf_C':[1.0,10.0,100.0]}\n",
    "             ]\n",
    "lr_tfidf = Pipeline([('vect',tfidf), ('clf',LogisticRegression(random_state = 0))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid, scoring = 'accuracy', cv = 2, verbose = 1, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [nonsense?, kiss, off,, geek., I, said, true.,...\n",
       "1       [\", pleas, vandal, pages,, thi, edit, W., S., ...\n",
       "2       [\", \"\"point, interest\"\", I, remov, \"\"point, in...\n",
       "3       [ask, hi, nation, racial, offence., wow, awar,...\n",
       "4       [reader, go, say, ether, vocal, style, dark, l...\n",
       "5            [fri, chicken, Is, dat, sum, fri, chickens?]\n",
       "6       [whi, put, english, exampl, player, peopl, lik...\n",
       "7       [guy, fawk, im, resid, bridgwat, go, carniv, e...\n",
       "8       [far, nicknam, go, thi, articl, embarrassing,,...\n",
       "9          [woodland, meadow, good, hear, correct, that.]\n",
       "10      [\", well, I, finish, good, bit, editing., I, c...\n",
       "11      [discuss, take, place, articl, talk, page,, us...\n",
       "12      [Uh, oh,, call, bluff., I, intimid, number, im...\n",
       "13      [\", We, also, contact, live, descend, adolf, h...\n",
       "14      [\", may, 2008, (utc), notabl, new, heart, A, t...\n",
       "15      [\", I, agre, thi, articl, FA, material,, I, mu...\n",
       "16      [turkish, citizen, receiv, award(notability), ...\n",
       "17      [pleas, explain, whi, censorship, qualiti, add...\n",
       "18      [In, ani, case,, thi, edit, war, last, forever...\n",
       "19      [\", \"\"vandalism\"\", georg, washington, say, cha...\n",
       "20      [whi, alitalia, remov, rom, allianc, due, piss...\n",
       "21      [\", anoth, afd, stat, exampl, I, hope, mind, l...\n",
       "22      [\", I, ;)., about..., ah,, i'v, got, noth, do!...\n",
       "23      [\":i, move, tediou, detail, \"\"survey, helmet, ...\n",
       "24      [@anniehall,, separ, thi, capit, state, terror...\n",
       "25      [., also, random,, wa, first, clan/guild, star...\n",
       "26      [\", grace, slick...., Is, non, ungrac, dick!86...\n",
       "27      [\"====regard, edit, made, dure, decemb, 2, 200...\n",
       "28      [\"::the, section, call, \"\"discrep, inaccuracie...\n",
       "29      [\", smackdown!, good, smackdown, qatar,, \"\"......\n",
       "                              ...                        \n",
       "1470    [thi, content, dispute., it', user, conduct., ...\n",
       "1471    [My, person, opinion, Is, you'r, fuck, wanker,...\n",
       "1472    [right, also, mani, european, countri, trend, ...\n",
       "1473    [pleas, stop., If, continu, vandal, wikipedia,...\n",
       "1474    [I, can't, becaus, sign-in, ban, admin, god, w...\n",
       "1475    [post, talk, page, I, post, sotomayor, talk, p...\n",
       "1476    [yes...bad, faith, undo, edit, without, explan...\n",
       "1477    [\", cannot, go, 7-eleven, dunkin', donut, unle...\n",
       "1478                             [revert, stuff, please?]\n",
       "1479    [,, follow, user:makecat, aid, china!, grats!!...\n",
       "1480    [Oi, oi!, gave, permiss, revert, edit, user, t...\n",
       "1481                        [,, draft, virtual, secrecy,]\n",
       "1482    [\"read, \"\"power, horror\"\", tell, isnt, anti-se...\n",
       "1483    [pagan, christ, i'd, like, propos, section, cr...\n",
       "1484                    [angeliqu, carrington-h, jews!]]]\n",
       "1485    [Oh, beliv, responc, veri, warranted., obviou,...\n",
       "1486                            [silli, thi, final, warn]\n",
       "1487    [I, time, discuss, thing, committee., life, sh...\n",
       "1488    [\", climat, data, An, unregist, editor, keep, ...\n",
       "1489    [\", histori, snapshot, follow, snapshot, edit,...\n",
       "1490    [\", district, safe, hi,, regard, edit, dallata...\n",
       "1491    [it', true, far, goes,, establish, church, eng...\n",
       "1492    [\", Am, I, suppos, scared?, it', like, I, can'...\n",
       "1493                        [chang, bttf2, thing, though]\n",
       "1494    [respons, abov, criticism., I, first, person, ...\n",
       "1495    [By, click, “save, page”, button,, agre, term,...\n",
       "1496    [2006, bangkok, bomb, hey,, i'v, gone, 2006, b...\n",
       "1497    [correspond, data, transfer, rate, kilobits/, ...\n",
       "1498    [support, chri, lawson, sure, want, support, t...\n",
       "1499    [I, agree,, I, realli, see, anyon, could, real...\n",
       "Name: tokenized_re_apply, Length: 1500, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mult = MultiLabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on MultiLabelBinarizer in module sklearn.preprocessing.label object:\n",
      "\n",
      "class MultiLabelBinarizer(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin)\n",
      " |  Transform between iterable of iterables and a multilabel format\n",
      " |  \n",
      " |  Although a list of sets or tuples is a very intuitive format for multilabel\n",
      " |  data, it is unwieldy to process. This transformer converts between this\n",
      " |  intuitive format and the supported multilabel format: a (samples x classes)\n",
      " |  binary matrix indicating the presence of a class label.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  classes : array-like of shape [n_classes] (optional)\n",
      " |      Indicates an ordering for the class labels\n",
      " |  \n",
      " |  sparse_output : boolean (default: False),\n",
      " |      Set to true if output binary array is desired in CSR sparse format\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  classes_ : array of labels\n",
      " |      A copy of the `classes` parameter where provided,\n",
      " |      or otherwise, the sorted set of classes found when fitting.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.preprocessing import MultiLabelBinarizer\n",
      " |  >>> mlb = MultiLabelBinarizer()\n",
      " |  >>> mlb.fit_transform([(1, 2), (3,)])\n",
      " |  array([[1, 1, 0],\n",
      " |         [0, 0, 1]])\n",
      " |  >>> mlb.classes_\n",
      " |  array([1, 2, 3])\n",
      " |  \n",
      " |  >>> mlb.fit_transform([set(['sci-fi', 'thriller']), set(['comedy'])])\n",
      " |  array([[0, 1, 1],\n",
      " |         [1, 0, 0]])\n",
      " |  >>> list(mlb.classes_)\n",
      " |  ['comedy', 'sci-fi', 'thriller']\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  sklearn.preprocessing.OneHotEncoder : encode categorical integer features\n",
      " |      using a one-hot aka one-of-K scheme.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MultiLabelBinarizer\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, classes=None, sparse_output=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, y)\n",
      " |      Fit the label sets binarizer, storing `classes_`\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      y : iterable of iterables\n",
      " |          A set of labels (any orderable and hashable object) for each\n",
      " |          sample. If the `classes` parameter is set, `y` will not be\n",
      " |          iterated.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : returns this MultiLabelBinarizer instance\n",
      " |  \n",
      " |  fit_transform(self, y)\n",
      " |      Fit the label sets binarizer and transform the given label sets\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      y : iterable of iterables\n",
      " |          A set of labels (any orderable and hashable object) for each\n",
      " |          sample. If the `classes` parameter is set, `y` will not be\n",
      " |          iterated.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_indicator : array or CSR matrix, shape (n_samples, n_classes)\n",
      " |          A matrix such that `y_indicator[i, j] = 1` iff `classes_[j]` is in\n",
      " |          `y[i]`, and 0 otherwise.\n",
      " |  \n",
      " |  inverse_transform(self, yt)\n",
      " |      Transform the given indicator matrix into label sets\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      yt : array or sparse matrix of shape (n_samples, n_classes)\n",
      " |          A matrix containing only 1s ands 0s.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : list of tuples\n",
      " |          The set of labels for each sample such that `y[i]` consists of\n",
      " |          `classes_[j]` for each `yt[i, j] == 1`.\n",
      " |  \n",
      " |  transform(self, y)\n",
      " |      Transform the given label sets\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      y : iterable of iterables\n",
      " |          A set of labels (any orderable and hashable object) for each\n",
      " |          sample. If the `classes` parameter is set, `y` will not be\n",
      " |          iterated.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_indicator : array or CSR matrix, shape (n_samples, n_classes)\n",
      " |          A matrix such that `y_indicator[i, j] = 1` iff `classes_[j]` is in\n",
      " |          `y[i]`, and 0 otherwise.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help (mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of MultiLabelBinarizer(classes=None, sparse_output=False)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mult.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.set_params of MultiLabelBinarizer(classes=None, sparse_output=False)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mult.set_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_mult =mult.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 48 candidates, totalling 96 fits\n"
     ]
    }
   ],
   "source": [
    "gs_lr_tfidf.fit(x_train_mult, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
