{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# importing neccessary libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting classifier"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "suppose you have trained a few classifier, each one achieving about 80% accuracy.\n",
    "You may have a logistic regression classifier, an SVM classifier, a random forest classifier, a K-nearest neighbor\n",
    "classifier and perhaps a few more.\n",
    "\n",
    "A very simple way to create a even better classifier is to aggregate the predictions of each classifier\n",
    "and predict the class that gets the most votes.\n",
    "\n",
    "Somewhat, the voting classifier often achieves a higher accuracy than the best classifier in the ensemble.\n",
    "\n",
    "In fact, even if each classifier is a weak learner (meaning it does only slightly better than random guessing), \n",
    "the ensemble can still be a strong learner (achieving a higher accuracy), provided there are a sufficient number\n",
    "of weak learner and they sufficiently diverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following code creates and train a voting classifier in sklearn, composed of 3 divers classfier on the moon dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "x, y = make_moons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 2), (100,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, train_size = 0.8, random_state = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)), ('rf', RandomF...,\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))],\n",
       "         n_jobs=1, voting='hard', weights=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(estimators = [('lr',log_clf), ('rf',rnd_clf),('svc',svm_clf)], voting = 'hard')\n",
    "\n",
    "voting_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.95\n",
      "RandomForestClassifier 0.95\n",
      "SVC 0.95\n",
      "VotingClassifier 0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf,rnd_clf,svm_clf, voting_clf):\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print (clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# soft voting"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if all classifiers are able to estimate class probablities (i.e, they have a predict_proba method), they you can tell sklearn to predict the class with the \n",
    "highest class probability, averaged over all the individual classifiers.\n",
    "\n",
    "we have soft voting by replacing voting = 'hard' with 'soft'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)), ('rf', RandomF...',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))],\n",
       "         n_jobs=1, voting='soft', weights=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC(probability = True)\n",
    "\n",
    "voting_clf = VotingClassifier(estimators = [('lr',log_clf), ('rf',rnd_clf),('svc',svm_clf)], voting = 'soft')\n",
    "\n",
    "voting_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.95\n",
      "RandomForestClassifier 1.0\n",
      "SVC 0.95\n",
      "VotingClassifier 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf,rnd_clf,svm_clf, voting_clf):\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print (clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging and pasting"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "It involves using the same training algorithms for every predictor, but to train\n",
    "them on different random subsets of the training set.\n",
    "\n",
    "Bagging involves sampling with replacement\n",
    "pasting involves sampling without replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bagging and pasting in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following code trains an ensemble of 500 decision tree classifier, each\n",
    "# training instances randomly sampled from the training set with replacement\n",
    "\n",
    "# to use pasting set bootstrap =false\n",
    "\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators = 500,\n",
    "                          max_samples = 70, bootstrap = True, n_jobs = -1)\n",
    "bag_clf.fit(x_train, y_train)\n",
    "y_pred = bag_clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.95\n",
      "RandomForestClassifier 1.0\n",
      "SVC 0.95\n",
      "BaggingClassifier 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf,rnd_clf,svm_clf, bag_clf):\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print (clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# out of bag evaluation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with bagging, some instances may be sampled several times for any given predictor, while others may not be sampled at all. The precentage of unsampled\n",
    "data are called out-of-bag(oob).\n",
    "\n",
    "Since a predictor never sees the oobs instances during training, it can be evaluated on these instances, without the need for a separate validation set or cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96250000000000002"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting oob_score = True while creating a bagging classifier to request an \n",
    "# automatic oob evaluation after training.\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators = 500,\n",
    "                           bootstrap = True, n_jobs = -1, oob_score = True)\n",
    "bag_clf.fit(x_train, y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = bag_clf.predict(x_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.        ],\n",
       "       [ 0.1       ,  0.9       ],\n",
       "       [ 0.19459459,  0.80540541],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.93922652,  0.06077348],\n",
       "       [ 0.70430108,  0.29569892],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.64835165,  0.35164835],\n",
       "       [ 0.83240223,  0.16759777],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.12568306,  0.87431694],\n",
       "       [ 0.80412371,  0.19587629],\n",
       "       [ 0.15116279,  0.84883721],\n",
       "       [ 0.37931034,  0.62068966],\n",
       "       [ 0.95321637,  0.04678363],\n",
       "       [ 0.31609195,  0.68390805],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.98469388,  0.01530612],\n",
       "       [ 0.60106383,  0.39893617],\n",
       "       [ 0.06417112,  0.93582888],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.51381215,  0.48618785],\n",
       "       [ 0.01257862,  0.98742138],\n",
       "       [ 0.95625   ,  0.04375   ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.05076142,  0.94923858],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.21212121,  0.78787879],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.77368421,  0.22631579],\n",
       "       [ 0.27173913,  0.72826087],\n",
       "       [ 0.86046512,  0.13953488],\n",
       "       [ 0.44387755,  0.55612245],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.03664921,  0.96335079],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.99438202,  0.00561798],\n",
       "       [ 0.16167665,  0.83832335],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.84      ,  0.16      ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.27428571,  0.72571429],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.08333333,  0.91666667],\n",
       "       [ 0.74556213,  0.25443787],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.72      ,  0.28      ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.0989011 ,  0.9010989 ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.09770115,  0.90229885],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.04519774,  0.95480226],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.05524862,  0.94475138],\n",
       "       [ 0.37765957,  0.62234043],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.08227848,  0.91772152],\n",
       "       [ 0.95530726,  0.04469274],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.00581395,  0.99418605]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The bagging classifier class support sampling the features as well. This is controlled by two hyperparameters: max_features and bootstrap_features. They work the same way as max_samples and bootstrap, but for feature sampling instead of instance sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random forest classifier"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " random forest are optimized for decisiontrees\n",
    " with few exceptions, a randomforestclassifier has all the hyperparameters of a decisiontreeclassifier(to control how trees are grown), plus all the hyperparameters of a bagging classifier to control the ensemble itself.\n",
    " \n",
    " Random forest algorithm introduces extra randomness when growing trees; instead of searching for the best feature when spliting a node. It searches for the best feature among a random subset of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_clf = RandomForestClassifier(n_estimators = 500, max_leaf_nodes = 16, n_jobs = -1)\n",
    "rnd_clf.fit(x_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following bagging classifier is roughly equivalent to the previous randomfores classifier\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(splitter = 'random', max_leaf_nodes = 16),\n",
    "                           n_estimators = 500, max_samples = 1.0, bootstrap = True ,n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.fit(x_train, y_train)\n",
    "rand_bag_clf = bag_clf.predict(x_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXtra Tree classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A forest of extremely random trees is simple called an extremely randomized tree ensemble (or extra trees for short)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "extr_cls = ExtraTreesClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature importance with randomforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.0937453105343\n",
      "sepal width (cm) 0.0232503397491\n",
      "petal length (cm) 0.416869406156\n",
      "petal width (cm) 0.46613494356\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators = 500, n_jobs = -1)\n",
    "rnd_clf.fit(iris['data'], iris['target'])\n",
    "for name, score in zip(iris['feature_names'], rnd_clf.feature_importances_):\n",
    "    print (name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting (originally called hypothesis boosting) refers to any ensemble method that can combine several weak learners into a strong learner.\n",
    "The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way for a new predictor to correct its predecessor is to pay a bit more attention to the training instances that the predecessor underfitted. This results in new predictors focusing more and more on the hard cases. this is the technique used by Adaboost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted error rate of the Jth predictor\n",
    "\n",
    "def weighted(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "def weighted_error_rate(data,target, predictor):\n",
    "    no_of_samples = data.shape[0]\n",
    "    predict_value = predictor.predict(data)\n",
    "    weighted_value = [weighted(i) if i != j else int(1) for i,j in zip(predict_value, target)]\n",
    "    normal_weight = [weighted(i) for i in predict_value]\n",
    "    solution = [a/b for a,b in zip(weighted_value, normal_weight)]\n",
    "    return solution\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predicators_weight(data,target,predictor,learning_param = 0.3):\n",
    "    compute_1 = weighted_error_rate(data,target,predictor)\n",
    "    compute_2 = np.log((1-compute_1)/compute_1)\n",
    "    return learning_param * compute_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def alpha(val):\n",
    "    return val\n",
    "\n",
    "def updated_weights(predicted_value, value):\n",
    "    if predicted_value == value:\n",
    "        val = weighted(predicted_value)\n",
    "    else:\n",
    "        val = weighted(predicted_value) * np.exp(alpha(value))\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "          learning_rate=0.5, n_estimators=200, random_state=None)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth = 1), n_estimators = 200, \n",
    "                            algorithm = 'SAMME.R', learning_rate = 0.5)\n",
    "ada_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking the instance weights at every iteration like AdaBoost does, this method tries to fit the new predictor to the residual errors made by the previous predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "x, y = None, None\n",
    "\n",
    "\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth = 2)\n",
    "\n",
    "tree_reg1.fit(x,y)\n",
    "\n",
    "y2 = y - tree_reg1.predict(x)\n",
    "\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth = 2)\n",
    "\n",
    "tree_reg2.fit(x, y2)\n",
    "\n",
    "y3 = y2 - tree_reg2.predict(x)\n",
    "\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth = 2)\n",
    "\n",
    "tree_reg3.fit(x,y3)\n",
    "\n",
    "\n",
    "y_pred = sum(tree.predict(x_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import GradientBoostRegressor\n",
    "\n",
    "gbrt = GradientBoostRegressor(max_depth = 2, n_estimators = 3, learning_rate = 1.0)\n",
    "gbrt.fit(x,y)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# the following doe trains a GBRT ensemble with 120 trees, then measures the validation\n",
    "# error at each stage of training to find the optimal number of trees, and finally trains\n",
    "# another gbrt ensemble using the optimal number of trees.\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(X,y)\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth = 2, n_estimators = 120)\n",
    "gbrt.fit(x_train, y_train)\n",
    "\n",
    "errors = [mean_squared_error(y_val, y_pred) for y_pred in gbrt.staged_predict(x_val)]\n",
    "bst_n_estimators = np.argmin(errors)\n",
    "\n",
    "gbrt_best = GradientBoostRegressor(max_depth = 2, n_estimators =bst_n_estimators)\n",
    "gbrt_best.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " it is also possible to implement early stopping training early (instead of training\n",
    " a large number of trees first and then looking back to find the optimal number).\n",
    " we can do by setting warm_start = True\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth = 2, warm_start = True)\n",
    "\n",
    "min_val_error = float('inf')\n",
    "\n",
    "error_going_up = 0\n",
    "\n",
    "for n_estimators in range(1,120):\n",
    "\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    \n",
    "    gbrt.fit(x_train, y_train)\n",
    "    \n",
    "    y_pred = gbrt.predict(x_val)\n",
    "    \n",
    "    val_error = mean_squared_error(y_val, y_pred)\n",
    "    \n",
    "    if val_error < min_val_error:\n",
    "    \n",
    "        min_val_error = val_error\n",
    "        \n",
    "        error_going_up = 0\n",
    "        \n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        \n",
    "        if error_going_up == 5:\n",
    "        \n",
    "            break # early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
